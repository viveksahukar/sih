{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "dicom_to_png.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPir9/BRi4hA0sHARnIAeok",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/viveksahukar/sih/blob/main/dicom_to_png.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UiuSujBqXJss"
      },
      "source": [
        "# This notebook details how the dicom files are converted to png that would be used as input to CNN."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8YbzREFKXGNE"
      },
      "source": [
        "# Import required libraries\n",
        "from vs165_modules import *"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BN2btp5BXX4Q"
      },
      "source": [
        "## Reading axial slices first..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wd6XfSupXGZY"
      },
      "source": [
        "# Reading the csv file\n",
        "# use r string in file path since space in file folder name\n",
        "ax = pd.read_csv(r\"ax_t1spgr+c_series_master_copy.csv\")\n",
        "ax['label'] = np.where(ax['dirpath'].str.contains('Positive', case=False, regex=True), 1, 0)\n",
        "\n",
        "ax.rename(columns={'dirpath':'fpath'}, inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PDhcDoX4XGcK"
      },
      "source": [
        "# Reading dicom files for each series, selecting a particular slice and saving them for each Patient in a new DataFrame\n",
        "series_df_all = pd.DataFrame()\n",
        "columns = ['PatientID', 'SOPInstanceUID', 'InstanceNumber']\n",
        "# series_df_all = pd.DataFrame(columns = ['PatientID', 'SOPInstanceUID', 'InstanceNumber'])\n",
        "for index, row in ax.iterrows():\n",
        "    series_df = pd.DataFrame()\n",
        "    inputdir = row.fpath\n",
        "    slices = [f for f in os.listdir(inputdir)]\n",
        "    data = []\n",
        "    for f in slices:\n",
        "        ds = pydicom.dcmread(inputdir + '/' + f)\n",
        "        values = [ds.PatientID, ds.SOPInstanceUID, ds.InstanceNumber]\n",
        "        zipped = zip(columns, values)\n",
        "        a_dictionary = dict(zipped)\n",
        "        data.append(a_dictionary)\n",
        "        series_df = series_df.append(a_dictionary, True)\n",
        "    img_count = len(series_df)\n",
        "    target = math.floor(img_count * 0.7) # Slice selection heuristic - gives one slide that contains most important feature for SIH\n",
        "    series_df = series_df[series_df['InstanceNumber'] == target]\n",
        "    series_df_all = series_df_all.append(series_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "78Oa6AuWXfbs"
      },
      "source": [
        "# Check for any mismatch in keys during merging of the above two dataframes\n",
        "\"\"\"\n",
        "These are the 2 patient id - 1 each in positive and negative case, where the slices in the series folder  \n",
        "have duplicate Instance Number and hence when floor(image_count * 0.7) is taken, the value is outside the range of InstanceNumber\n",
        "\"\"\"\n",
        "ax2 = ax.merge(series_df_all, how='outer', indicator='True')\n",
        "ax2[ax2['True'] != 'both']\n",
        "ax2.info()\n",
        "# For now leaving these two PatientID, putting them in test set and proceed as usual"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZjEeUBwBXfYh"
      },
      "source": [
        "# Merging the above two dataframes\n",
        "ax_new = ax.merge(series_df_all, how='inner', on='PatientID')\n",
        "ax_new['full_fpath'] = ax_new['fpath'] + '/' + ax_new['SOPInstanceUID'] + '.dcm'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "REqK3ro8XfVm"
      },
      "source": [
        "# Saving ax_new to a dataframe and then csv, so all steps till here need not be repeated\n",
        "ax_new.to_csv(r'ax_new.csv', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VUaDcCgcXqBU"
      },
      "source": [
        "## Now reading coronal slices - similar method as for axial slices...."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o9W3koSrXfPl"
      },
      "source": [
        "# Reading the csv file\n",
        "# use r string in file path since space in file folder name\n",
        "cor = pd.read_csv(r\"cor_t1spgr+c_series_master_copy.csv\")\n",
        "cor['label'] = np.where(cor['dirpath'].str.contains('Positive', case=False, regex=True), 1, 0)\n",
        "\n",
        "cor.rename(columns={'dirpath':'fpath'}, inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2SJxJaRxXfMv"
      },
      "source": [
        "# Reading dicom files for each series, selecting a particular slice and saving them for each Patient in a new DataFrame\n",
        "series_df_all_cor = pd.DataFrame()\n",
        "columns = ['PatientID', 'SOPInstanceUID', 'InstanceNumber']\n",
        "# series_df_all = pd.DataFrame(columns = ['PatientID', 'SOPInstanceUID', 'InstanceNumber'])\n",
        "for index, row in cor.iterrows():\n",
        "    series_df_cor = pd.DataFrame()\n",
        "    inputdir = row.fpath\n",
        "    slices = [f for f in os.listdir(inputdir)]\n",
        "    data = []\n",
        "    for f in slices:\n",
        "        ds = pydicom.dcmread(inputdir + '/' + f)\n",
        "        values = [ds.PatientID, ds.SOPInstanceUID, ds.InstanceNumber]\n",
        "        zipped = zip(columns, values)\n",
        "        a_dictionary = dict(zipped)\n",
        "        data.append(a_dictionary)\n",
        "        series_df_cor = series_df_cor.append(a_dictionary, True)\n",
        "    img_count = len(series_df_cor)\n",
        "    target1 = math.floor(img_count * 0.3)     # Slice selection heuristic - We are choosing 2 slices for each series\n",
        "    target2 = math.floor(img_count * 0.625)\n",
        "    series_df_cor = series_df_cor[(series_df_cor['InstanceNumber'] == target1) | (series_df_cor['InstanceNumber'] == target2)]\n",
        "    series_df_all_cor = series_df_all_cor.append(series_df_cor)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EvR5uaniXfJw"
      },
      "source": [
        "# Check for any mismatch in keys during merging of the above two dataframes\n",
        "cor2 = cor.merge(series_df_all_cor, how='outer', indicator='True')\n",
        "cor2 = cor2[cor2['True'] != 'both']\n",
        "cor2.info()\n",
        "# All good...."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5HRlt3g-XfHA"
      },
      "source": [
        "# Saving cor_new to a df and then csv, so all steps till here not to be repeated\n",
        "cor_new.to_csv(r'cor_new.csv', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NwUWe675X_JR"
      },
      "source": [
        "## Now loading axial dataframe that contains one slice for each series for each PatientID and preparing the train, test and validation sets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U8fQxqEkXfEK"
      },
      "source": [
        "#Load csv dataframe\n",
        "df_ax = pd.read_csv(r'ax_new.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N2fqKheSXfBA"
      },
      "source": [
        "# Separating into positive and negative cases\n",
        "ax_pos = df_ax[df_ax.label == 1]\n",
        "ax_neg = df_ax[df_ax.label == 0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gBGqBhXBXe-N"
      },
      "source": [
        "# to check if any duplicates in PatientID\n",
        "boolean = ax_pos['PatientID'].is_unique\n",
        "boolean = ax_neg['PatientID'].is_unique \n",
        "# no duplicates found"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9kxoDAzzYIeB"
      },
      "source": [
        "#### **Number of patients for positive and negative cases in train, validation and test sets**\n",
        "| Case | Train | Validation | Test | Total |\n",
        "| :-- | --: | --: | --: | --: |\n",
        "| Axial - Positive | 51 | 34 | 85 | 170 |\n",
        "| Axial - Negative | 93 | 61 | 154 | 308 |\n",
        "| Axial - Total | 144 | 95 | 239 | 478 |\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t-3_DdVfXe7A"
      },
      "source": [
        "# Dividing into different dataframes as per above table\n",
        "ax_pos_train = ax_pos.iloc[:51, :]\n",
        "ax_pos_val = ax_pos.iloc[51:85, :]\n",
        "ax_pos_test = ax_pos.iloc[85:, :]\n",
        "\n",
        "ax_neg_train = ax_neg.iloc[:93, :]\n",
        "ax_neg_val = ax_neg.iloc[93:154, :]\n",
        "ax_neg_test = ax_neg.iloc[154:, :]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Nt4KIHvXe4l"
      },
      "source": [
        "# Creating data directories for storing png files\n",
        "ax_pos_train_dir = 'data_ax/ax_train/1/'  \n",
        "ax_pos_val_dir = 'data_ax/ax_val/1/'\n",
        "ax_pos_test_dir = 'data_ax/ax_test/1/' \n",
        "\n",
        "ax_neg_train_dir = 'data_ax/ax_train/0/'  \n",
        "ax_neg_val_dir = 'data_ax/ax_val/0/'\n",
        "ax_neg_test_dir = 'data_ax/ax_test/0/' "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eJRVo6qrXe1E"
      },
      "source": [
        "IMG_PX_SIZE = 224 # set image size to 224 x 224 - resnet standards\n",
        "def get_png(df, folder):\n",
        "# Convert dcm to png using imageio and saves png in respective folders\n",
        "    for index, row in df.iterrows():\n",
        "        inputdir = row.full_fpath\n",
        "        ds = pydicom.dcmread(inputdir)\n",
        "        brks = ds.scaled_px.freqhist_bins(n_bins=256)\n",
        "        ds_scaled = ds.hist_scaled(brks=brks, min_px=50)\n",
        "        img = cv2.resize(np.array(ds_scaled), (IMG_PX_SIZE, IMG_PX_SIZE))\n",
        "        img = np.repeat(img[..., np.newaxis], 3, -1) # copying across other 2 dim to make 3d image\n",
        "        rescaled = ((255.0 / img.max() * (img - img.min()))).astype(np.uint8)\n",
        "        imageio.imwrite(folder + str(row.PatientID) + '_' + row.SOPInstanceUID + '.png', rescaled) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "II9AKHToYYH6"
      },
      "source": [
        "# Applying above function to convert each dicom slice per PatientID to png and saving it in respective folders\n",
        "get_png(ax_pos_train, ax_pos_train_dir)\n",
        "get_png(ax_neg_train, ax_neg_train_dir)\n",
        "get_png(ax_pos_val, ax_pos_val_dir)\n",
        "get_png(ax_neg_val, ax_neg_val_dir)\n",
        "get_png(ax_pos_test, ax_pos_test_dir)\n",
        "get_png(ax_neg_test, ax_neg_test_dir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J0qC5etUYYD-"
      },
      "source": [
        "def create_label(folder, label_filename, save_location):\n",
        "    # Create label file for slices, takes input the data directory to be traversed, name of csv filename, and path location where csv is to be saved\n",
        "    df = pd.DataFrame(columns=['img', 'label'])\n",
        "    for root, dir, files in os.walk(folder):\n",
        "        files = [f for f in files if not f.startswith('~')]\n",
        "        df1 = pd.DataFrame({'img': files, 'label': 1})\n",
        "        df = df.append(df1)\n",
        "    df.to_csv(save_location + label_filename + '.csv', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vRV5hxBQYX_q"
      },
      "source": [
        "# Create label files for each dataset\n",
        "axial_data_dir =  'data_ax_cor/data_ax/'\n",
        "create_label(ax_pos_train_dir, 'ax_pos_train_label', axial_data_dir)\n",
        "create_label(ax_pos_val_dir, 'ax_pos_val_label', axial_data_dir)\n",
        "create_label(ax_pos_test_dir, 'ax_pos_test_label', axial_data_dir)\n",
        "\n",
        "create_label(ax_neg_train_dir, 'ax_neg_train_label', axial_data_dir)\n",
        "create_label(ax_neg_val_dir, 'ax_neg_val_label', axial_data_dir)\n",
        "create_label(ax_neg_test_dir, 'ax_neg_test_label', axial_data_dir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RGzWL6tEYjRX"
      },
      "source": [
        "## Now, repeating the above same steps - histogram normalization and saving to png for coronal axis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4GRY5Ys4YX7W"
      },
      "source": [
        "#Load csv dataframe for axial images\n",
        "df_cor = pd.read_csv(r'test/cor_new.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8RkPX0T0YX3o"
      },
      "source": [
        "# Separating into positive and negative cases\n",
        "cor_pos = df_cor[df_cor.label == 1]\n",
        "cor_neg = df_cor[df_cor.label == 0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PMAgtMQnYtiv"
      },
      "source": [
        "### Keeping Train (30%), Validation (20%), Test (50%) for each positive and negative cases\n",
        "\n",
        "#### **Number of slices - 2 for each patient for positive and negative cases in train, validation and test sets**\n",
        "| Case | Train | Validation | Test | Total |\n",
        "| :-- | --: | --: | --: | --: |\n",
        "| Coronal - Positive | 100 | 64 | 162 | 326 |\n",
        "| Coronal - Negative | 166 | 110 | 274 | 550 |\n",
        "| Coronal - Total | 266 | 174 | 436 | 876 |"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zg2C8RevYXzb"
      },
      "source": [
        "# Dividing into different dataframes as per above table\n",
        "\n",
        "cor_pos_train = cor_pos.iloc[:100, :]\n",
        "cor_pos_val = cor_pos.iloc[100:164, :]\n",
        "cor_pos_test = cor_pos.iloc[164:, :]\n",
        "\n",
        "cor_neg_train = cor_neg.iloc[:166, :]\n",
        "cor_neg_val = cor_neg.iloc[166:276, :]\n",
        "cor_neg_test = cor_neg.iloc[276:, :]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iz8Y-1UcYXt1"
      },
      "source": [
        "# Checked that all dataframes have unique PatientID and no PatientID is present in more than one set.\n",
        "df = cor_neg_test.merge(cor_neg_val, how='outer', indicator='True', on='PatientID')\n",
        "df['True'].value_counts()\n",
        "\n",
        "# Similarly check for all combinations\n",
        "\n",
        "# All good....."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E7Toe9mJYXj-"
      },
      "source": [
        "# Creating data directories for storing png files\n",
        "cor_pos_train_dir = 'cor_train/1/'  \n",
        "cor_pos_val_dir = 'cor_val/1/'\n",
        "cor_pos_test_dir = 'cor_test/1/' \n",
        "\n",
        "cor_neg_train_dir = 'cor_train/0/'  \n",
        "cor_neg_val_dir = 'cor_val/0/'\n",
        "cor_neg_test_dir = 'cor_test/0/' "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xT0usWhMYXW9"
      },
      "source": [
        "# Applying dicom to png function to convert each dicom slice per PatientID to png and saving it in respective folders\n",
        "get_png(cor_pos_train, cor_pos_train_dir)\n",
        "get_png(cor_neg_train, cor_neg_train_dir)\n",
        "get_png(cor_pos_val, cor_pos_val_dir)\n",
        "get_png(cor_neg_val, cor_neg_val_dir)\n",
        "get_png(cor_pos_test, cor_pos_test_dir)\n",
        "get_png(cor_neg_test, cor_neg_test_dir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vUnbip75Y7Bm"
      },
      "source": [
        "# Create label files for each dataset\n",
        "coronal_data_dir =  'data_cor/'\n",
        "create_label(cor_pos_train_dir, 'cor_pos_train_label', coronal_data_dir)\n",
        "create_label(cor_pos_val_dir, 'cor_pos_val_label', coronal_data_dir)\n",
        "create_label(cor_pos_test_dir, 'cor_pos_test_label', coronal_data_dir)\n",
        "\n",
        "create_label(cor_neg_train_dir, 'cor_neg_train_label', coronal_data_dir)\n",
        "create_label(cor_neg_val_dir, 'cor_neg_val_label', coronal_data_dir)\n",
        "create_label(cor_neg_test_dir, 'cor_neg_test_label', coronal_data_dir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LRaRNpRVY-_w"
      },
      "source": [
        "### Checking Patients that have both axial and coronal slices"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZgK3u7KZY68_"
      },
      "source": [
        "# Dropping duplicate patient ID in coronal and axial dataframes\n",
        "df_ax_unique = df_ax.drop_duplicates(subset=['PatientID'])\n",
        "df_cor_unique = df_cor.drop_duplicates(subset=['PatientID'])\n",
        "\n",
        "df_cor_unique_pos = df_cor_unique[df_cor_unique.label == 1]\n",
        "df_cor_unique_neg = df_cor_unique[df_cor_unique.label == 0]\n",
        "df_ax_unique_pos = df_ax_unique[df_ax_unique.label == 1]\n",
        "df_ax_unique_neg = df_ax_unique[df_ax_unique.label == 0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TG4gFWqAY63Z"
      },
      "source": [
        "# Checked for common Patients in entire axial and coronal datasets\n",
        "df = df_ax_unique.merge(df_cor_unique, how='outer', indicator='True', on='PatientID')\n",
        "df['True'].value_counts()\n",
        "# All good....."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lmfdxOU_Y6yp"
      },
      "source": [
        "# Checked for common Patients in axial and coronal datasets - for positive cases only\n",
        "df_pos = df_ax_unique_pos.merge(df_cor_unique_pos, how='outer', indicator='True', on='PatientID')\n",
        "df_pos['True'].value_counts()\n",
        "# All good....."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OzvFdYZ0Y6vT"
      },
      "source": [
        "# Checked for common Patients in axial and coronal datasets - for negative cases only\n",
        "df_neg = df_ax_unique_neg.merge(df_cor_unique_neg, how='outer', indicator='True', on='PatientID')\n",
        "df_neg['True'].value_counts()\n",
        "# All good....."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "At-xw-OmZJ_R"
      },
      "source": [
        "#### **Number of patients having slices for both axes - axial and coronal in positive and negative cases**\n",
        "| Case | Both Axial & Coronal | Only Axial | Only Coronal |\n",
        "| :-- | --: | --: | --: |\n",
        "| Positive | 155 | 15 | 9 |\n",
        "| Negative | 273 | 35 | 3 |\n",
        "| Total | 428 | 50 | 12 |"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gp7Av0feZNo5"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}